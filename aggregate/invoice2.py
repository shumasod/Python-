#!/usr/bin/env python3
â€œâ€â€
å–å¼•æ˜ç´°é›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ”¹å–„ç‰ˆï¼‰
Features:

- å …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
- è¨­å®šã®å¤–éƒ¨åŒ–
- è©³ç´°ãªåˆ†ææ©Ÿèƒ½
- ã‚°ãƒ©ãƒ•ç”Ÿæˆ
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
- ãƒ†ã‚¹ãƒˆå¯èƒ½ãªè¨­è¨ˆ
  â€œâ€â€

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Union
import logging
import argparse
import json
import yaml
from dataclasses import dataclass
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
from decimal import Decimal, InvalidOperation
import warnings

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š

plt.rcParams[â€˜font.familyâ€™] = [â€˜DejaVu Sansâ€™, â€˜Hiragino Sansâ€™, â€˜Yu Gothicâ€™, â€˜Meiryoâ€™, â€˜Takaoâ€™, â€˜IPAexGothicâ€™, â€˜IPAPGothicâ€™, â€˜VL PGothicâ€™, â€˜Noto Sans CJK JPâ€™]

@dataclass
class AnalysisConfig:
â€œâ€â€œåˆ†æè¨­å®šã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹â€â€â€
input_file: str
output_dir: str
date_column: str = â€˜å–å¼•æ—¥â€™
amount_column: str = â€˜é‡‘é¡â€™
category_column: str = â€˜ã‚«ãƒ†ã‚´ãƒªâ€™
description_column: str = â€˜å†…å®¹â€™
encoding: str = â€˜utf-8â€™
decimal_places: int = 0
generate_charts: bool = True
chart_style: str = â€˜seaborn-v0_8â€™
outlier_threshold: float = 3.0  # æ¨™æº–åå·®ã®å€æ•°

```
@classmethod
def from_file(cls, config_path: str) -> 'AnalysisConfig':
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
    config_path = Path(config_path)
    if config_path.suffix.lower() == '.json':
        with open(config_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    elif config_path.suffix.lower() in ['.yml', '.yaml']:
        with open(config_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
    else:
        raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {config_path.suffix}")
    
    return cls(**data)
```

class DataValidator:
â€œâ€â€œãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ç”¨ã‚¯ãƒ©ã‚¹â€â€â€

```
def __init__(self, config: AnalysisConfig):
    self.config = config
    self.logger = logging.getLogger(__name__)

def validate_dataframe(self, df: pd.DataFrame) -> Tuple[bool, List[str]]:
    """DataFrameã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    issues = []
    
    # å¿…é ˆåˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    required_columns = [
        self.config.date_column,
        self.config.amount_column,
        self.config.category_column
    ]
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        issues.append(f"å¿…é ˆåˆ—ãŒä¸è¶³: {missing_columns}")
    
    if not issues:  # åŸºæœ¬çš„ãªåˆ—ãŒã‚ã‚‹å ´åˆã®ã¿è©³ç´°ãƒã‚§ãƒƒã‚¯
        # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        if not pd.api.types.is_datetime64_any_dtype(df[self.config.date_column]):
            try:
                pd.to_datetime(df[self.config.date_column])
            except (ValueError, TypeError):
                issues.append(f"{self.config.date_column}åˆ—ã‚’æ—¥ä»˜ã¨ã—ã¦å¤‰æ›ã§ãã¾ã›ã‚“")
        
        # é‡‘é¡åˆ—ã®æ•°å€¤ãƒã‚§ãƒƒã‚¯
        if not pd.api.types.is_numeric_dtype(df[self.config.amount_column]):
            try:
                pd.to_numeric(df[self.config.amount_column], errors='coerce')
            except (ValueError, TypeError):
                issues.append(f"{self.config.amount_column}åˆ—ã‚’æ•°å€¤ã¨ã—ã¦å¤‰æ›ã§ãã¾ã›ã‚“")
        
        # ç©ºãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
        if df.empty:
            issues.append("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        
        # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
        duplicates = df.duplicated()
        if duplicates.any():
            duplicate_count = duplicates.sum()
            issues.append(f"é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãŒ{duplicate_count}ä»¶ã‚ã‚Šã¾ã™")
        
        # ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
        if self.config.amount_column in df.columns:
            numeric_amounts = pd.to_numeric(df[self.config.amount_column], errors='coerce')
            if not numeric_amounts.isna().all():
                z_scores = np.abs((numeric_amounts - numeric_amounts.mean()) / numeric_amounts.std())
                outliers = z_scores > self.config.outlier_threshold
                if outliers.any():
                    outlier_count = outliers.sum()
                    issues.append(f"ç•°å¸¸å€¤ãŒ{outlier_count}ä»¶æ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
    
    return len(issues) == 0, issues
```

class TransactionAnalyzer:
â€œâ€â€œå–å¼•æ˜ç´°åˆ†æã‚¯ãƒ©ã‚¹ï¼ˆæ”¹å–„ç‰ˆï¼‰â€â€â€

```
def __init__(self, config: AnalysisConfig):
    self.config = config
    self.input_file = Path(config.input_file)
    self.output_dir = Path(config.output_dir)
    self.df: Optional[pd.DataFrame] = None
    self.validator = DataValidator(config)
    self._setup_logging()
    self._ensure_output_dir()

def _setup_logging(self) -> None:
    """ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(
                self.output_dir / f'analysis_{datetime.now().strftime("%Y%m%d")}.log',
                encoding='utf-8'
            )
        ]
    )
    self.logger = logging.getLogger(__name__)

def _ensure_output_dir(self) -> None:
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
    self.output_dir.mkdir(parents=True, exist_ok=True)
    self.logger.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")

def load_data(self) -> None:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã‚’å®Ÿè¡Œ"""
    if not self.input_file.exists():
        raise FileNotFoundError(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.input_file}")
    
    try:
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {self.input_file}")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        self.df = pd.read_csv(
            self.input_file,
            encoding=self.config.encoding,
            parse_dates=[self.config.date_column],
            date_parser=lambda x: pd.to_datetime(x, errors='coerce')
        )
        
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df)}ä»¶")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        is_valid, issues = self.validator.validate_dataframe(self.df)
        
        if issues:
            self.logger.warning("ãƒ‡ãƒ¼ã‚¿å“è³ªã®å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ:")
            for issue in issues:
                self.logger.warning(f"  - {issue}")
        
        if not is_valid:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        self._clean_data()
        
    except Exception as e:
        self.logger.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        raise

def _clean_data(self) -> None:
    """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†"""
    initial_count = len(self.df)
    
    # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤
    self.df = self.df.drop_duplicates()
    
    # æ¬ æå€¤ã®å‡¦ç†
    self.df = self.df.dropna(subset=[
        self.config.date_column,
        self.config.amount_column,
        self.config.category_column
    ])
    
    # é‡‘é¡åˆ—ã®æ•°å€¤å¤‰æ›
    self.df[self.config.amount_column] = pd.to_numeric(
        self.df[self.config.amount_column], 
        errors='coerce'
    )
    self.df = self.df.dropna(subset=[self.config.amount_column])
    
    # æ—¥ä»˜åˆ—ã®å‡¦ç†
    self.df[self.config.date_column] = pd.to_datetime(
        self.df[self.config.date_column], 
        errors='coerce'
    )
    self.df = self.df.dropna(subset=[self.config.date_column])
    
    cleaned_count = len(self.df)
    removed_count = initial_count - cleaned_count
    
    if removed_count > 0:
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {removed_count}ä»¶ã®ä¸æ­£ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤")

def analyze_by_category(self) -> pd.DataFrame:
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´°é›†è¨ˆ"""
    category_stats = self.df.groupby(self.config.category_column)[self.config.amount_column].agg([
        ('åˆè¨ˆé‡‘é¡', 'sum'),
        ('å–å¼•ä»¶æ•°', 'count'),
        ('å¹³å‡é‡‘é¡', 'mean'),
        ('ä¸­å¤®å€¤', 'median'),
        ('æœ€å¤§é‡‘é¡', 'max'),
        ('æœ€å°é‡‘é¡', 'min'),
        ('æ¨™æº–åå·®', 'std')
    ]).round(self.config.decimal_places)
    
    # å‰²åˆã‚’è¿½åŠ 
    total_amount = self.df[self.config.amount_column].sum()
    category_stats['æ§‹æˆæ¯”(%)'] = (category_stats['åˆè¨ˆé‡‘é¡'] / total_amount * 100).round(1)
    
    return category_stats.sort_values('åˆè¨ˆé‡‘é¡', ascending=False)

def analyze_by_month(self) -> pd.DataFrame:
    """æœˆåˆ¥é›†è¨ˆï¼ˆè©³ç´°ç‰ˆï¼‰"""
    monthly_data = self.df.set_index(self.config.date_column).resample('M').agg({
        self.config.amount_column: ['sum', 'count', 'mean'],
        self.config.category_column: 'nunique'
    })
    
    # åˆ—åã‚’å¹³å¦åŒ–
    monthly_data.columns = ['åˆè¨ˆé‡‘é¡', 'å–å¼•ä»¶æ•°', 'å¹³å‡é‡‘é¡', 'ã‚«ãƒ†ã‚´ãƒªæ•°']
    
    # å‰æœˆæ¯”ã‚’è¿½åŠ 
    monthly_data['å‰æœˆæ¯”(%)'] = monthly_data['åˆè¨ˆé‡‘é¡'].pct_change() * 100
    
    return monthly_data.round(self.config.decimal_places)

def analyze_by_weekday(self) -> pd.DataFrame:
    """æ›œæ—¥åˆ¥é›†è¨ˆ"""
    weekday_names = ['æœˆæ›œæ—¥', 'ç«æ›œæ—¥', 'æ°´æ›œæ—¥', 'æœ¨æ›œæ—¥', 'é‡‘æ›œæ—¥', 'åœŸæ›œæ—¥', 'æ—¥æ›œæ—¥']
    
    self.df['æ›œæ—¥'] = self.df[self.config.date_column].dt.dayofweek
    weekday_stats = self.df.groupby('æ›œæ—¥')[self.config.amount_column].agg([
        ('åˆè¨ˆé‡‘é¡', 'sum'),
        ('å–å¼•ä»¶æ•°', 'count'),
        ('å¹³å‡é‡‘é¡', 'mean')
    ]).round(self.config.decimal_places)
    
    weekday_stats.index = [weekday_names[i] for i in weekday_stats.index]
    return weekday_stats

def analyze_trends(self) -> Dict:
    """ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
    # æœˆæ¬¡ãƒˆãƒ¬ãƒ³ãƒ‰
    monthly_amounts = self.df.set_index(self.config.date_column).resample('M')[self.config.amount_column].sum()
    
    # æˆé•·ç‡ã®è¨ˆç®—
    growth_rates = monthly_amounts.pct_change().dropna()
    
    # å­£ç¯€æ€§åˆ†æï¼ˆå››åŠæœŸåˆ¥ï¼‰
    self.df['å››åŠæœŸ'] = self.df[self.config.date_column].dt.quarter
    quarterly_stats = self.df.groupby('å››åŠæœŸ')[self.config.amount_column].agg(['sum', 'mean'])
    
    return {
        'monthly_trend': monthly_amounts,
        'growth_rates': growth_rates,
        'quarterly_stats': quarterly_stats,
        'trend_summary': {
            'å¹³å‡æˆé•·ç‡(%)': growth_rates.mean() * 100,
            'æœ€å¤§æˆé•·ç‡(%)': growth_rates.max() * 100,
            'æœ€å°æˆé•·ç‡(%)': growth_rates.min() * 100,
            'ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘': 'UP' if growth_rates.mean() > 0 else 'DOWN'
        }
    }

def detect_anomalies(self) -> pd.DataFrame:
    """ç•°å¸¸å€¤æ¤œå‡º"""
    # Z-scoreæ–¹å¼
    z_scores = np.abs((self.df[self.config.amount_column] - self.df[self.config.amount_column].mean()) / 
                     self.df[self.config.amount_column].std())
    
    anomalies = self.df[z_scores > self.config.outlier_threshold].copy()
    anomalies['Z-Score'] = z_scores[z_scores > self.config.outlier_threshold]
    
    return anomalies.sort_values('Z-Score', ascending=False)

def generate_summary(self) -> Dict:
    """åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼æƒ…å ±ã®ç”Ÿæˆ"""
    date_range = self.df[self.config.date_column].max() - self.df[self.config.date_column].min()
    
    summary = {
        'åŸºæœ¬çµ±è¨ˆ': {
            'ç·å–å¼•ä»¶æ•°': len(self.df),
            'ç·å–å¼•é‡‘é¡': self.df[self.config.amount_column].sum(),
            'å¹³å‡å–å¼•é‡‘é¡': self.df[self.config.amount_column].mean(),
            'ä¸­å¤®å€¤': self.df[self.config.amount_column].median(),
            'æœ€å¤§å–å¼•é‡‘é¡': self.df[self.config.amount_column].max(),
            'æœ€å°å–å¼•é‡‘é¡': self.df[self.config.amount_column].min(),
            'æ¨™æº–åå·®': self.df[self.config.amount_column].std(),
        },
        'æœŸé–“æƒ…å ±': {
            'åˆ†æé–‹å§‹æ—¥': self.df[self.config.date_column].min(),
            'åˆ†æçµ‚äº†æ—¥': self.df[self.config.date_column].max(),
            'åˆ†ææœŸé–“(æ—¥)': date_range.days,
            '1æ—¥å¹³å‡å–å¼•é¡': self.df[self.config.amount_column].sum() / max(date_range.days, 1)
        },
        'ã‚«ãƒ†ã‚´ãƒªæƒ…å ±': {
            'ã‚«ãƒ†ã‚´ãƒªæ•°': self.df[self.config.category_column].nunique(),
            'æœ€å¤šã‚«ãƒ†ã‚´ãƒª': self.df[self.config.category_column].mode().iloc[0] if not self.df[self.config.category_column].mode().empty else 'N/A'
        }
    }
    
    return summary

def create_charts(self) -> Dict[str, str]:
    """ã‚°ãƒ©ãƒ•ç”Ÿæˆ"""
    if not self.config.generate_charts:
        return {}
    
    chart_paths = {}
    
    # ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
    plt.style.use(self.config.chart_style)
    
    # 1. ã‚«ãƒ†ã‚´ãƒªåˆ¥å††ã‚°ãƒ©ãƒ•
    fig, ax = plt.subplots(figsize=(10, 8))
    category_data = self.df.groupby(self.config.category_column)[self.config.amount_column].sum()
    ax.pie(category_data.values, labels=category_data.index, autopct='%1.1f%%', startangle=90)
    ax.set_title('ã‚«ãƒ†ã‚´ãƒªåˆ¥å–å¼•é‡‘é¡æ§‹æˆæ¯”')
    chart_path = self.output_dir / 'category_pie_chart.png'
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    plt.close()
    chart_paths['category_pie'] = str(chart_path)
    
    # 2. æœˆåˆ¥æ¨ç§»ã‚°ãƒ©ãƒ•
    fig, ax = plt.subplots(figsize=(12, 6))
    monthly_data = self.df.set_index(self.config.date_column).resample('M')[self.config.amount_column].sum()
    ax.plot(monthly_data.index, monthly_data.values, marker='o', linewidth=2, markersize=6)
    ax.set_title('æœˆåˆ¥å–å¼•é‡‘é¡æ¨ç§»')
    ax.set_xlabel('æœˆ')
    ax.set_ylabel('å–å¼•é‡‘é¡')
    ax.grid(True, alpha=0.3)
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
    plt.xticks(rotation=45)
    chart_path = self.output_dir / 'monthly_trend.png'
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    plt.close()
    chart_paths['monthly_trend'] = str(chart_path)
    
    # 3. ã‚«ãƒ†ã‚´ãƒªåˆ¥æ£’ã‚°ãƒ©ãƒ•
    fig, ax = plt.subplots(figsize=(12, 8))
    category_data = self.analyze_by_category()['åˆè¨ˆé‡‘é¡'].head(10)
    bars = ax.bar(range(len(category_data)), category_data.values)
    ax.set_title('ã‚«ãƒ†ã‚´ãƒªåˆ¥å–å¼•é‡‘é¡ãƒˆãƒƒãƒ—10')
    ax.set_xlabel('ã‚«ãƒ†ã‚´ãƒª')
    ax.set_ylabel('å–å¼•é‡‘é¡')
    ax.set_xticks(range(len(category_data)))
    ax.set_xticklabels(category_data.index, rotation=45, ha='right')
    
    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, value in zip(bars, category_data.values):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(category_data.values)*0.01,
               f'{value:,.0f}', ha='center', va='bottom')
    
    chart_path = self.output_dir / 'category_bar_chart.png'
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    plt.close()
    chart_paths['category_bar'] = str(chart_path)
    
    return chart_paths

def export_results(self) -> str:
    """é›†è¨ˆçµæœã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    try:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = self.output_dir / f'å–å¼•é›†è¨ˆ_è©³ç´°_{timestamp}.xlsx'
        
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            # ã‚µãƒãƒªãƒ¼
            summary_data = self.generate_summary()
            summary_rows = []
            for category, items in summary_data.items():
                summary_rows.append([category, '', ''])
                for key, value in items.items():
                    summary_rows.append(['', key, value])
                summary_rows.append(['', '', ''])
            
            summary_df = pd.DataFrame(summary_rows, columns=['ã‚«ãƒ†ã‚´ãƒª', 'é …ç›®', 'å€¤'])
            summary_df.to_excel(writer, sheet_name='ã‚µãƒãƒªãƒ¼', index=False)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
            category_summary = self.analyze_by_category()
            category_summary.to_excel(writer, sheet_name='ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ')
            
            # æœˆåˆ¥é›†è¨ˆ
            monthly_summary = self.analyze_by_month()
            monthly_summary.to_excel(writer, sheet_name='æœˆåˆ¥é›†è¨ˆ')
            
            # æ›œæ—¥åˆ¥é›†è¨ˆ
            weekday_summary = self.analyze_by_weekday()
            weekday_summary.to_excel(writer, sheet_name='æ›œæ—¥åˆ¥é›†è¨ˆ')
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            trend_data = self.analyze_trends()
            trend_summary_df = pd.DataFrame.from_dict(
                trend_data['trend_summary'], 
                orient='index', 
                columns=['å€¤']
            )
            trend_summary_df.to_excel(writer, sheet_name='ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ')
            
            # ç•°å¸¸å€¤
            anomalies = self.detect_anomalies()
            if not anomalies.empty:
                anomalies.to_excel(writer, sheet_name='ç•°å¸¸å€¤', index=False)
            
            # ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
            sample_data = self.df.head(1000) if len(self.df) > 1000 else self.df
            sample_data.to_excel(writer, sheet_name='ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«', index=False)
        
        self.logger.info(f"è©³ç´°é›†è¨ˆçµæœã‚’ä¿å­˜: {output_file}")
        
        # ã‚°ãƒ©ãƒ•ç”Ÿæˆ
        if self.config.generate_charts:
            chart_paths = self.create_charts()
            self.logger.info(f"ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ: {len(chart_paths)}ä»¶")
        
        return str(output_file)
        
    except Exception as e:
        self.logger.error(f"çµæœå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
        raise
```

def create_sample_config(config_path: str) -> None:
â€œâ€â€œã‚µãƒ³ãƒ—ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆâ€â€â€
sample_config = {
â€œinput_fileâ€: â€œtransactions.csvâ€,
â€œoutput_dirâ€: â€œoutputâ€,
â€œdate_columnâ€: â€œå–å¼•æ—¥â€,
â€œamount_columnâ€: â€œé‡‘é¡â€,
â€œcategory_columnâ€: â€œã‚«ãƒ†ã‚´ãƒªâ€,
â€œdescription_columnâ€: â€œå†…å®¹â€,
â€œencodingâ€: â€œutf-8â€,
â€œdecimal_placesâ€: 0,
â€œgenerate_chartsâ€: True,
â€œchart_styleâ€: â€œseaborn-v0_8â€,
â€œoutlier_thresholdâ€: 3.0
}

```
config_path = Path(config_path)
if config_path.suffix.lower() == '.json':
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(sample_config, f, indent=2, ensure_ascii=False)
else:
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(sample_config, f, default_flow_style=False, allow_unicode=True)
```

def main():
â€œâ€â€œãƒ¡ã‚¤ãƒ³å‡¦ç†â€â€â€
parser = argparse.ArgumentParser(description=â€˜å–å¼•æ˜ç´°é›†è¨ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ”¹å–„ç‰ˆï¼‰â€™)
parser.add_argument(â€™â€“configâ€™, â€˜-câ€™, help=â€˜è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹â€™)
parser.add_argument(â€™â€“inputâ€™, â€˜-iâ€™, help=â€˜å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«â€™)
parser.add_argument(â€™â€“outputâ€™, â€˜-oâ€™, help=â€˜å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªâ€™)
parser.add_argument(â€™â€“create-configâ€™, help=â€˜ã‚µãƒ³ãƒ—ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆâ€™)
parser.add_argument(â€™â€“no-chartsâ€™, action=â€˜store_trueâ€™, help=â€˜ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚’ç„¡åŠ¹åŒ–â€™)

```
args = parser.parse_args()

# ã‚µãƒ³ãƒ—ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
if args.create_config:
    create_sample_config(args.create_config)
    print(f"ã‚µãƒ³ãƒ—ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {args.create_config}")
    return

try:
    # è¨­å®šã®èª­ã¿è¾¼ã¿
    if args.config:
        config = AnalysisConfig.from_file(args.config)
    else:
        config = AnalysisConfig(
            input_file=args.input or 'transactions.csv',
            output_dir=args.output or 'output'
        )
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§è¨­å®šã‚’ä¸Šæ›¸ã
    if args.input:
        config.input_file = args.input
    if args.output:
        config.output_dir = args.output
    if args.no_charts:
        config.generate_charts = False
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = TransactionAnalyzer(config)
    
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    analyzer.load_data()
    
    print("ğŸ“Š åˆ†æå®Ÿè¡Œä¸­...")
    output_file = analyzer.export_results()
    
    print(f"âœ… åˆ†æå®Œäº†! çµæœãƒ•ã‚¡ã‚¤ãƒ«: {output_file}")
    
    # ç°¡å˜ãªã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    summary = analyzer.generate_summary()
    print("\nğŸ“‹ åˆ†æã‚µãƒãƒªãƒ¼:")
    print(f"  - ç·å–å¼•ä»¶æ•°: {summary['åŸºæœ¬çµ±è¨ˆ']['ç·å–å¼•ä»¶æ•°']:,}ä»¶")
    print(f"  - ç·å–å¼•é‡‘é¡: Â¥{summary['åŸºæœ¬çµ±è¨ˆ']['ç·å–å¼•é‡‘é¡']:,.0f}")
    print(f"  - åˆ†ææœŸé–“: {summary['æœŸé–“æƒ…å ±']['åˆ†ææœŸé–“(æ—¥)']}æ—¥é–“")
    print(f"  - ã‚«ãƒ†ã‚´ãƒªæ•°: {summary['ã‚«ãƒ†ã‚´ãƒªæƒ…å ±']['ã‚«ãƒ†ã‚´ãƒªæ•°']}å€‹")
    
except Exception as e:
    logging.error(f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    raise
```

if **name** == â€œ**main**â€:
main()